<h1 align="center">
  <br>
  AI Detector
  <br>
</h1>

<h4 align="center">Detect AI generated coding answers</h4>

<p align="center">
  <a href="https://ai-detector-scopic.vercel.app">
    <img src="https://img.shields.io/badge/demo-YouTube-red.svg">
  </a>
  <a href="https://ai-detector-scopic.vercel.app">
    <img src="https://img.shields.io/badge/live-vercel-blue.svg">
  </a>
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/license-MIT-yellow.svg">
  </a>
</p>

## Table of Contents
<ul>
  <li><a href="#problem-statement">Problem Statement</a></li>
  <li><a href="#data-understanding">Data Understanding</a></li>
  <li><a href="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#model-training">Model Training</a></li>
  <li><a href="#model-compression">Model Compression</a></li>
  <li><a href="#model-deployment">Model Deployment</a></li>
  <li><a href="#web-deployment">Web Deployment</a></li>
  <li><a href="#build-from-source">Build from Source</a></li>
  <li><a href="#contact">Contact</a></li>
</ul>

## Problem Statement

The objective of this project is to develop a Machine Learning model that can detect potential AI use by comparing candidate coding answers to responses generated by AI models (GPT-4, GPT-4 Turbo, and GPT-3.5 Turbo).

## Data Understanding

The original dataset contains examples of coding questions, candidate answers, AI-generated answers, and the corresponding AI-detected scores to train and test the model. The goal is for the model to predict AI-detected scores for new, unseen data. The dataset can be found in the [`data`](/data/) directory of this project. The dataset has one directory and one file as follows:

1. [`dataset-source-codes`](/data/dataset-source-codes/) directory: This directory has 63 subdirectories (`source_code_000` ... `source_code_062`). Each of these subdirectories represents a coding question and its respective answers completed by both candidate and AI. A subdirectory, say, `source_code_000` has the following 8 files:
   
  - `source_code_000.json`: Contains a coding question in a specific programing language (Java in this case) and metadata related to that question
  - `source_code_000.jav`: Contains candidate's answer code snippet written in Java
  - `source_code_000_gpt-3.5-turbo_00.jav` and `...01.jav`: These two files have two samples of the respective coding answer completed by GPT-3.5-Turbo
  - `source_code_000_gpt-4_00.jav` and `...01.jav`: These two files have two samples of the respective coding answer completed by GPT-4-Turbo
  - `source_code_000_gpt-4-turbo_00.jav` and `...01.jav`: These two files have two samples of the respective coding answer completed by GPT-4-Turbo
  
2. [`CodeAid Source Codes Labeling.xlsx`](/data/CodeAid%20Source%20Codes%20Labeling.xlsx): This file maps a candidate's answers to its respective AI-generated answers and assigns a plagiarism score. It has 3 columns and 378 rows as follows:

| |coding_problem_id | llm_answer_id | plagiarism_score|
| --- | ------------ | ------------ | ------------- |
| 1 |source_code_000 | gpt-3.5-turbo_00 | 0|
| 2 | source_code_000 | gpt-3.5-turbo_01 | 0|
| 3 |source_code_000	| gpt-4_00 | 0|
| ... | ... | ... | ... |
| 378 |source_code_062 | gpt-4-turbo_01 | 0.30|

## Data Preprocessing

We need to preprocess the data to build a consistent dataset structure for the model training and validation. Preprocessing involves the follwoing steps:

1. Load all 63 subdirectories containing the coding questions and answers from the `dataset-source-codes` directory
2. Load `CodeAid Source Codes Labeling.xlsx` file with plagiarism scores
3. Create a new tabular dataset where each row has a coding question, respective candidate answer, AI-generated answer, and the associated plagiarism score. This step creates 378 rows. Since, there are 6 different plagiarism/similarity score for a candidate's answer (2 AI-generated answers for 3 different LLMs, so 63 * 2 * 3 = 378 rows)
4. Adds two new columns combining coding question with candidate answer and AI-generated answer respectively (This is necessary for feature extraction purposes)

The detailed preprocessing documentation can be found in [`preprocessing.ipynb`](/notebooks/preprocessing.ipynb) Jupyter Notebook or [`preprocessing.py`](/scripts/preprocessing.py) file. The preprocessed data with 6 columns and 378 rows is as follows:

| 	| question | candidate_answer |	ai_answer | similarity_score | candidate_combined | ai_combined |
| --- | --- | --- | --- | --- | --- | --- |
| 1	| Write a program to find the largest element in... | fun findLargestElement(array: IntArray) : Int ... | public class LargestElementFinder {\n publi... | 0.0 | Question: Write a program to find the largest ... | Question: Write a program to find the largest ... |
| 2	| Write a program to find the largest element in... |	fun findLargestElement(array: IntArray) : Int ... |	public class Main {\n public static void ma... | 0.0 | Question: Write a program to find the largest ... | Question: Write a program to find the largest ... |
| 3	| Write a program to find the largest element in... |	fun findLargestElement(array: IntArray) : Int ... |	public class Main {\n public static void main... |	0.0	| Question: Write a program to find the largest ... |	Question: Write a program to find the largest ... |
| ... |	... | ... |	... |	... |	...	| ... |	... |
| 378	| Create a PHP script that will accept a string ... |	<?php\nfunction getTopThreeWords($text) {\n// ... |	<?php\n\n// Function to get the top three most... |	0.3	| Question: Create a PHP script that will accept... |	Question: Create a PHP script that will accept... |

The preprocessed data is saved as [preprocessed_data.csv](/data/preprocessed_data.csv) file.

## Model Training

5 different Sentence Transformers were selected for fine-tuning based on their average performance on sentence encoding from [sbert.net](https://www.sbert.net). The model training step involves the follwoing:

1. Splits the preprocessed data into training and validation data
2. Specifies data feature columns and label column for the training data. This is where feature extraction takes place (coding answers are converted to embeddings)
3. Loads training data into **DataLoaders** with hyperparameter `batch_size`
4. Defines the Sentence Transformer model architecture
5. Defines the loss function as `CosineSimilarityLoss`
6. Defines an evaluator with validation data
7. Finally, fine-tunes the model over the specified hyperparameter `epochs`

The detailed model training documentation can be found in [`train.ipynb`](/notebooks/train.ipynb) Jupyter Notebook or [`train.py`](/scripts/train.py) file. The fine-tuned model is saved at [`models`](/models/) directory.

## Model Evaluation



## Model Deployment

## Web Deployment

## Build from Source

1. Clone the repo

```bash
git clone https://github.com/zzarif/AI-Detector.git
cd AI-Detector/
```

2. Initialize and activate virtual environment

```bash
virtualenv venv
source venv/Scripts/activate
```

3. Install dependencies

```bash
pip install -r requirements.txt
```

_Note: Select virtual environment interpreter from_ `Ctrl`+`Shift`+`P`

4. Preprocess the Data

Run all the cells in [`preprocessing.ipynb`](/notebooks/preprocessing.ipynb) Jupyter Notebook or run the following script:

```bash
python preprocessing.py
```

5. Train the model

Run all the cells in [`train.ipynb`](/notebooks/train.ipynb) Jupyter Notebook or run the following script (with specified model):

```bash
python train.py --model all-MiniLM-L6-v2
```

6. Evaluate the model

Run all the cells in [`evaluation.ipynb`](/notebooks/evaluation.ipynb) Jupyter Notebook or run the following script (with the specified fine-tuned model):

```bash
python evaluation.py --model fine-tuned_all-MiniLM-L6-v2
```

7. Perform model inference

Run all the cells in [`inference.ipynb`](/notebooks/inference.ipynb) Jupyter Notebook or run the following script (with the specified fine-tuned model):

```bash
python inference.py --model fine-tuned_all-MiniLM-L6-v2
```

## Miscellaneous



## Contact

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?logo=linkedin&logoColor=white)](https://www.linkedin.com/in/zibran-zarif-amio-b82717263/) [![Mail](https://img.shields.io/badge/Gmail-EA4335?logo=gmail&logoColor=fff)](mailto:zibran.zarif.amio@gmail.com)
