<h1 align="center">
  <br>
  AI Detector
  <br>
</h1>

<h4 align="center">Detect AI generated coding answers</h4>

<p align="center">
  <a href="https://ai-detector-scopic.vercel.app">
    <img src="https://img.shields.io/badge/demo-YouTube-red.svg">
  </a>
  <a href="https://ai-detector-scopic.vercel.app">
    <img src="https://img.shields.io/badge/live-vercel-blue.svg">
  </a>
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/license-MIT-yellow.svg">
  </a>
</p>

## Table of Contents

<ul>
  <li><a href="#problem-statement">Problem Statement</a></li>
  <li><a href="#data-understanding">Data Understanding</a></li>
  <li><a href="#data-preprocessing">Data Preprocessing</a></li>
  <li><a href="#model-training">Model Training</a></li>
  <li><a href="#model-evaluation">Model Evaluation</a></li>
  <li><a href="#model-deployment">Model Deployment</a></li>
  <li><a href="#web-deployment">Web Deployment</a></li>
  <li><a href="#build-from-source">Build from Source</a></li>
  <li><a href="#contact">Contact</a></li>
</ul>

## Problem Statement

The objective of this project is to develop a Machine Learning model that can detect potential AI use by comparing candidate coding answers to responses generated by AI models (GPT-4, GPT-4 Turbo, and GPT-3.5 Turbo). The model will predict AI-detected score ranging in floating point numbers from `0` (no AI detected) to `1` (a lot of AI-detected). Since the prediction is a continuous value in a range, this is a **Regression** problem which can be solved in multiple ways. In this problem's context, I will demonstrate **two** solutions:

- **Solution 1:** Fine-tuning multiple Sentence Transformers on the given dataset. The best performing fine-tuned model was deployed to [HuggingFace](https://huggingface.co/spaces/zzarif/AI-Detector) and integrated with a [Flask Webapp](https://ai-detector-scopic.vercel.app/).
- **Solution 2:** Utilizing Large Language Models to predict similarity score.

## Data Understanding

The original dataset contains examples of coding questions, candidate answers, AI-generated answers, and the corresponding AI-detected scores to train and test the model. The goal is for the model to predict AI-detected scores for new, unseen data. The dataset can be found in the [`data`](/data/) directory of this project. The dataset has one directory and one file as follows:

1. [`dataset-source-codes`](/data/dataset-source-codes/) directory: This directory has 63 subdirectories (`source_code_000` ... `source_code_062`). Each of these subdirectories represents a coding question and its respective answers completed by both candidate and AI. A subdirectory, say, `source_code_000` has the following 8 files:

- `source_code_000.json`: Contains a coding question in a specific programing language (Java in this case) and metadata related to that question
- `source_code_000.jav`: Contains candidate's answer code snippet written in Java
- `source_code_000_gpt-3.5-turbo_00.jav` and `...01.jav`: These two files have two samples of the respective coding answer completed by GPT-3.5-Turbo
- `source_code_000_gpt-4_00.jav` and `...01.jav`: These two files have two samples of the respective coding answer completed by GPT-4
- `source_code_000_gpt-4-turbo_00.jav` and `...01.jav`: These two files have two samples of the respective coding answer completed by GPT-4-Turbo

2. [`CodeAid Source Codes Labeling.xlsx`](/data/CodeAid%20Source%20Codes%20Labeling.xlsx): This file maps a candidate's answers to its respective AI-generated answers and assigns a plagiarism score. It has 3 columns and 378 rows as follows:

<table>
    <thead>
        <tr>
            <th></th>
            <th>coding_problem_id</th>
            <th>llm_answer_id</th>
            <th>plagiarism_score</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1</td>
            <td>source_code_000</td>
            <td>gpt-3.5-turbo_00</td>
            <td>0</td>
        </tr>
        <tr>
            <td>2</td>
            <td>source_code_000</td>
            <td>gpt-3.5-turbo_01</td>
            <td>0</td>
        </tr>
        <tr>
            <td>3</td>
            <td>source_code_000</td>
            <td>gpt-4_00</td>
            <td>0</td>
        </tr>
        <tr>
            <td>...</td>
            <td>...</td>
            <td>...</td>
            <td>...</td>
        </tr>
        <tr>
            <td>378</td>
            <td>source_code_062</td>
            <td>gpt-4-turbo_01</td>
            <td>0.30</td>
        </tr>
    </tbody>
</table>

## Data Preprocessing

We need to preprocess the data to build a consistent dataset structure for the model training and validation. Preprocessing involves the follwoing steps:

1. Load all 63 subdirectories containing the coding questions and answers from the `dataset-source-codes` directory
2. Load `CodeAid Source Codes Labeling.xlsx` file with plagiarism scores
3. Create a new tabular dataset where each row has a coding question, respective candidate answer, AI-generated answer, and the associated plagiarism score. This step creates 378 rows. Since, there are 6 different plagiarism/similarity score for a candidate's answer (2 AI-generated answers for 3 different LLMs, so 63 * 2 * 3 = 378 rows)
4. Adds two new columns combining coding question with candidate answer and AI-generated answer respectively (This is necessary for feature extraction purposes)

The preprocessed data with 6 columns and 378 rows is as follows:

<table>
    <thead>
        <tr>
            <th></th>
            <th>question</th>
            <th>candidate_answer</th>
            <th>ai_answer</th>
            <th>similarity_score</th>
            <th>candidate_combined</th>
            <th>ai_combined</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1</td>
            <td>Write a program to find...</td>
            <td>fun findLargestElement...</td>
            <td>public class LargestEle...</td>
            <td>0.0</td>
            <td>Question: Write a program...</td>
            <td>Question: Write a program...</td>
        </tr>
        <tr>
            <td>2</td>
            <td>Write a program to find...</td>
            <td>fun findLargestElement...</td>
            <td>public class Main {\n ...</td>
            <td>0.0</td>
            <td>Question: Write a program...</td>
            <td>Question: Write a program...</td>
        </tr>
        <tr>
            <td>3</td>
            <td>Write a program to find...</td>
            <td>fun findLargestElement...</td>
            <td>public class Main {\n ...</td>
            <td>0.0</td>
            <td>Question: Write a program...</td>
            <td>Question: Write a program...</td>
        </tr>
        <tr>
            <td>...</td>
            <td>...</td>
            <td>...</td>
            <td>...</td>
            <td>...</td>
            <td>...</td>
            <td>...</td>
        </tr>
        <tr>
            <td>378</td>
            <td>Create a PHP script that will...</td>
            <td>&lt;?php\nfunction getTop...</td>
            <td>&lt;?php\n\n// Function...</td>
            <td>0.3</td>
            <td>Question: Create a PHP script...</td>
            <td>Question: Create a PHP script...</td>
        </tr>
    </tbody>
</table>

The detailed preprocessing documentation can be found in [`preprocessing.ipynb`](/notebooks/preprocessing.ipynb) Jupyter Notebook or [`preprocessing.py`](/scripts/preprocessing.py) file. The preprocessed data is saved as [`preprocessed_data.csv`](/data/preprocessed_data.csv) file.

## Model Training

5 different Sentence Transformers were selected for fine-tuning based on their average performance on sentence encoding from [sbert.net](https://www.sbert.net). The model training step involves the follwoing:

1. Splits the preprocessed data into training and validation data
2. Specifies data feature columns and label column for the training data. This is where feature extraction takes place (coding answers are converted to embeddings)
3. Loads training data into **DataLoaders** with hyperparameter `batch_size`
4. Defines the Sentence Transformer model architecture
5. Defines the loss function as `CosineSimilarityLoss`
6. Defines an evaluator with validation data
7. Finally, fine-tunes the model over the specified hyperparameter `epochs`

The detailed model training documentation can be found in [`train.ipynb`](/notebooks/train.ipynb) Jupyter Notebook or [`train.py`](/scripts/train.py) file. The fine-tuned model is saved at [`models`](/models/) directory.

## Model Evaluation

Following are the 8 different metrics utilized for evaluating 5 fine-tuned models:

<table>
    <thead>
        <tr>
            <th>Metric</th>
            <th>all-mpnet-base-v2</th>
            <th>all-distilroberta-v1</th>
            <th>all-MiniLM-L12-v2</th>
            <th>all-MiniLM-L6-v2</th>
            <th>multi-qa-mpnet-base-dot-v1</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Cosine Spearman</td>
            <td>0.9508</td>
            <td>0.9519</td>
            <td>0.8966</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Manhattan Spearman</td>
            <td>0.95</td>
            <td>0.9477</td>
            <td>0.8925</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Euclidean Spearman</td>
            <td>0.9508</td>
            <td>0.9519</td>
            <td>0.8966</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Dot Product Spearman</td>
            <td>0.9508</td>
            <td>0.9519</td>
            <td>0.8966</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Mean Squared Error</td>
            <td>0.0063</td>
            <td>0.0056</td>
            <td>0.0257</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Root Mean Squared Error</td>
            <td>0.0794</td>
            <td>0.0749</td>
            <td>0.1602</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Mean Absolute Error</td>
            <td>0.0583</td>
            <td>0.0534</td>
            <td>0.0954</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>R-squared Score</td>
            <td>0.9119</td>
            <td>0.9215</td>
            <td>0.6412</td>
            <td></td>
            <td></td>
        </tr>
    </tbody>
</table>

Key Insights from the evaluation metrics:

1. 

The detailed implementation can be found in [`evaluation.ipynb`](/notebooks/evaluation.ipynb) Jupyter Notebook or [`evaluation.py`](/scripts/evaluation.py) file.

## Model Deployment

From the model evaluation, it is evident that the fine-tuned `all-distilroberta-v1` model is relatively the best performing model. Hence, this model was deployed to [HuggingFace](https://huggingface.co/spaces/zzarif/AI-Detector) spaces with Gradio. Following is a snapshot of the HuggingFace app.

[attach image]

## Web Deployment

A custom [Web Application](https://ai-detector-scopic.vercel.app/) was developed with Flask to demonstrate the AI detection model's capability. It uses the HuggingFace Spaces API in the backend. Following are some snapshots of the Flask webapp:

[attach image]

## Large Language Model



## Build from Source

1. Clone the repo

```bash
git clone https://github.com/zzarif/AI-Detector.git
cd AI-Detector/
```

2. Initialize and activate virtual environment

```bash
virtualenv venv
source venv/Scripts/activate
```

3. Install dependencies

```bash
pip install -r requirements.txt
```

_Note: Select virtual environment interpreter from_ `Ctrl`+`Shift`+`P`

4. Preprocess the Data

Run all the cells in [`preprocessing.ipynb`](/notebooks/preprocessing.ipynb) Jupyter Notebook or run the following script:

```bash
python scripts/preprocessing.py
```

5. Train the model

Run all the cells in [`train.ipynb`](/notebooks/train.ipynb) Jupyter Notebook or run the following script (with specified model and hyperparameters):

```bash
python scripts/train.py --model all-MiniLM-L6-v2 --epochs 5 --batch_size 16
```

6. Evaluate the model

Run all the cells in [`evaluation.ipynb`](/notebooks/evaluation.ipynb) Jupyter Notebook or run the following script (with the specified fine-tuned model):

```bash
python scripts/evaluation.py --ft_model all-MiniLM-L6-v2
```

7. Perform model inference

Run all the cells in [`inference.ipynb`](/notebooks/inference.ipynb) Jupyter Notebook or run the following script (with the specified fine-tuned model):

```bash
python scripts/inference.py --ft_model all-MiniLM-L6-v2
```

8. Export model for deployment

Run all the cells in [`export.ipynb`](/notebooks/export.ipynb) Jupyter Notebook or run the following script (with the specified fine-tuned model):

```bash
python scripts/export.py --ft_model all-MiniLM-L6-v2
```

## Miscellaneous

The [utils](/utils/) directory contains some helper scripts. For example, [`file_extension_resolver.py`](/utils/file_extension_resolver.py) script parses the original dataset and creates the programming language to extension mapping dictionary used in [preprocessing.py](/scripts/preprocessing.py) file.

## Contact

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?logo=linkedin&logoColor=white)](https://www.linkedin.com/in/zibran-zarif-amio-b82717263/) [![Mail](https://img.shields.io/badge/Gmail-EA4335?logo=gmail&logoColor=fff)](mailto:zibran.zarif.amio@gmail.com)

Thank you so much for your interest. Would love your valuable feedback!