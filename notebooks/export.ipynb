{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Detector - Export Model\n",
    "\n",
    "Since, we have trained, evaluated, and did inference on our model, it's time to export it for deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries\n",
    "At first, let's import necessary libraries to convert our model to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define `export_to_onnx()` function\n",
    "- **Params:**\n",
    "  - `model_path` -> fine-tuned model path\n",
    "  - `onnx_path` -> converted ONNX model path\n",
    "\n",
    "This function converts the fine-tuned SBERT model to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_onnx(model_path, onnx_path):\n",
    "    # Load the SentenceTransformer model\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Prepare dummy input (adjust input size as needed)\n",
    "    inputs = tokenizer(\"Candidate coding answer\", \"AI generated answer\", padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Export the model\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        # Model input (tuple)\n",
    "        args=(inputs['input_ids'], inputs['attention_mask']),\n",
    "        f=onnx_path,  # Output ONNX file path\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"output\"],  # Name of the model output\n",
    "        dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"}, \n",
    "                    \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"}, \n",
    "                    \"output\": {0: \"batch_size\"}},\n",
    "        opset_version=11,\n",
    "    )\n",
    "\n",
    "    print(f\"Model exported to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define `quantize_onnx_model()` Function\n",
    "\n",
    "- **Params:**\n",
    "  - `onnx_path` -> Exported ONNX model path\n",
    "  - `quantized_onnx_path` -> Compressed ONNX model path\n",
    "\n",
    "This function compresses and optimizes the ONNX for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_onnx_model(onnx_path, quantized_onnx_path):\n",
    "    quantize_dynamic(onnx_path,\n",
    "                     quantized_onnx_path,\n",
    "                     weight_type=QuantType.QUInt8)  # 8-bit integers for weights\n",
    "    print(f\"Quantized model saved to: {quantized_onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert / Export the Model\n",
    "\n",
    "At first, define the fine-tuned model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-mpnet-base-v2\"\n",
    "# model_name = \"all-distilroberta-v1\"\n",
    "# model_name = \"all-MiniLM-L12-v2\"\n",
    "# model_name = \"all-MiniLM-L6-v2\"\n",
    "# model_name = \"multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define the necessary export directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_dir = data_dir = os.path.join(\n",
    "    os.path.abspath(''), os.pardir, 'models')\n",
    "model_path = os.path.join(model_dir, f\"fine-tuned_{model_name}\")\n",
    "\n",
    "onnx_path = os.path.join(model_dir, f\"fine-tuned_{model_name}.onnx\")\n",
    "\n",
    "quantize_onnx_path = os.path.join(\n",
    "    model_dir, f\"fine-tuned_{model_name}_quantized.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to e:\\Data Science\\AI-Detector\\notebooks\\..\\models\\fine-tuned_all-mpnet-base-v2.onnx\n"
     ]
    }
   ],
   "source": [
    "export_to_onnx(model_path, onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to: e:\\Data Science\\AI-Detector\\notebooks\\..\\models\\fine-tuned_all-mpnet-base-v2_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "quantize_onnx_model(onnx_path, quantize_onnx_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
