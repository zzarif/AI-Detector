{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Detector - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we need to import required libraries for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also specify `device` for GPU accelerated training (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define `train_model()` Function\n",
    "\n",
    "- **Params:** \n",
    "  - `df` -> The preprocessed data\n",
    "  - `model_name` -> The specified Sentence Transformer from [sbert.net](https://sbert.net)\n",
    "  - `output_path` -> Fine-tuned model export path\n",
    "  - `epochs` -> Number of iterations in the training loop (defaults to 5)\n",
    "  - `batch_size` -> Size of batches of training data (defaults to 16)\n",
    "- **Returns:** Nothing\n",
    "\n",
    "The functions performs the following operations:\n",
    "1. Splits the preprocessed data into training and validation data\n",
    "2. Specifies data feature columns and label column for the training data. This is where feature extraction takes place (coding answers are converted to embeddings).\n",
    "3. Loads training data into **DataLoaders** with ideal `batch_size`\n",
    "4. Defines the model architecture and assigns to the `device`\n",
    "5. Defines the loss function (`CosineSimilarityLoss` in this case)\n",
    "6. Defines an evaluator with validation data\n",
    "7. Finally, **Fine-tunes the specified SBERT model** and exports it to its dedicated directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, model_name, output_path, epochs=5, batch_size=16):\n",
    "    # Split the data into train and test sets\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create examples for training\n",
    "    train_examples = [InputExample(texts=[row['candidate_combined'], row['ai_combined']], label=float(\n",
    "        row['similarity_score'])) for _, row in train_df.iterrows()]\n",
    "\n",
    "    # Create DataLoader for training with appropriate batch size\n",
    "    train_dataloader = DataLoader(\n",
    "        train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Initialize the specified SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    # Define the loss function\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "    # Prepare validation data\n",
    "    valid_samples = [(row['candidate_combined'], row['ai_combined'], row['similarity_score'])\n",
    "                     for _, row in valid_df.iterrows()]\n",
    "    valid_examples = [InputExample(\n",
    "        texts=[s[0], s[1]], label=float(s[2])) for s in valid_samples]\n",
    "\n",
    "    # Create an evaluator\n",
    "    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "        valid_examples, name='validation')\n",
    "\n",
    "    # Train/fine-tune the model\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "              epochs=epochs,\n",
    "              warmup_steps=100,\n",
    "              evaluator=evaluator,\n",
    "              evaluation_steps=500,\n",
    "              output_path=output_path,\n",
    "              show_progress_bar=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model\n",
    "At first, we specify the Sentence Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"all-mpnet-base-v2\"\n",
    "# model_name = \"all-distilroberta-v1\"\n",
    "model_name = \"all-MiniLM-L12-v2\"\n",
    "# model_name = \"all-MiniLM-L6-v2\"\n",
    "# model_name = \"multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, specify the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we specify the data and model export directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data_dir = os.path.join(os.path.abspath(''), os.pardir, 'data')\n",
    "df = pd.read_csv(os.path.join(data_dir, 'preprocessed_data.csv'))\n",
    "\n",
    "# Define model export/output path\n",
    "model_dir = os.path.join(\n",
    "    os.path.abspath(''), os.pardir, 'models')\n",
    "output_path = os.path.join(model_dir, f'fine-tuned_{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zzami\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "  0%|          | 0/95 [00:00<?, ?it/s]e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 95/95 [00:44<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 44.1923, 'train_samples_per_second': 34.169, 'train_steps_per_second': 2.15, 'train_loss': 0.1801853380705181, 'epoch': 5.0}\n",
      "Model training complete. Model saved as e:\\Data Science\\AI-Detector\\notebooks\\..\\models\\fine-tuned_all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(df, model_name, output_path, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "print(f\"Model training complete. Model saved as {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
