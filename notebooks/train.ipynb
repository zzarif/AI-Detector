{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Detector - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we need to import required libraries for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also specify `device` for GPU accelerated training (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define `train_model()` Function\n",
    "\n",
    "- **Params:** \n",
    "  - `df` -> The preprocessed data\n",
    "  - `model_name` -> The specified Sentence Transformer from [sbert.net](https://sbert.net)\n",
    "  - `output_path` -> Fine-tuned model export path\n",
    "  - `epochs` -> Number of iterations in the training loop (defaults to 5)\n",
    "  - `batch_size` -> Size of batches of training data (defaults to 16)\n",
    "- **Returns:** Nothing\n",
    "\n",
    "The functions performs the following operations:\n",
    "1. Splits the preprocessed data into training and validation data\n",
    "2. Specifies data feature columns and label column for the training data. This is where feature extraction takes place (coding answers are converted to embeddings).\n",
    "3. Loads training data into **DataLoaders** with ideal `batch_size`\n",
    "4. Defines the model architecture and assigns to the `device`\n",
    "5. Defines the loss function (`CosineSimilarityLoss` in this case)\n",
    "6. Defines an evaluator with validation data\n",
    "7. Finally, **Fine-tunes the specified SBERT model** and exports it to its directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, model_name, output_path, epochs=5, batch_size=16):\n",
    "    # Split the data into train and test sets\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create examples for training\n",
    "    train_examples = [InputExample(texts=[row['candidate_combined'], row['ai_combined']], label=float(\n",
    "        row['similarity_score'])) for _, row in train_df.iterrows()]\n",
    "\n",
    "    # Create DataLoader for training with appropriate batch size\n",
    "    train_dataloader = DataLoader(\n",
    "        train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    # Initialize the specified SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    # Define the loss function\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "    # Prepare validation data\n",
    "    valid_samples = [(row['candidate_combined'], row['ai_combined'], row['similarity_score'])\n",
    "                     for _, row in valid_df.iterrows()]\n",
    "    valid_examples = [InputExample(\n",
    "        texts=[s[0], s[1]], label=float(s[2])) for s in valid_samples]\n",
    "\n",
    "    # Create an evaluator\n",
    "    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "        valid_examples, name='validation')\n",
    "\n",
    "    # Train/fine-tune the model\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "              epochs=epochs,\n",
    "              warmup_steps=100,\n",
    "              evaluator=evaluator,\n",
    "              evaluation_steps=500,\n",
    "              output_path=output_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model\n",
    "At first, specify the data and model export directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "data_dir = os.path.join(os.path.abspath(''), os.pardir, 'data')\n",
    "df = pd.read_csv(os.path.join(data_dir, 'preprocessed_data.csv'))\n",
    "\n",
    "# Define model export/output path\n",
    "model_dir = os.path.join(\n",
    "    os.path.abspath(''), os.pardir, 'models')\n",
    "output_path = os.path.join(model_dir, 'fine-tuned_all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 95/95 [04:56<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 296.6126, 'train_samples_per_second': 5.091, 'train_steps_per_second': 0.32, 'train_loss': 0.1601176613255551, 'epoch': 5.0}\n",
      "Model training complete. Model saved as e:\\Data Science\\AI-Detector\\notebooks\\..\\models\\fine-tuned_all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(df, 'all-MiniLM-L6-v2', output_path)\n",
    "\n",
    "print(f\"Model training complete. Model saved as {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
