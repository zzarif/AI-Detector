{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Detector - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Data Science\\AI-Detector\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also specify `device` for GPU accelerated training (if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def manhattan_distance(a, b):\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "def dot_product(a, b):\n",
    "    return np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define `evaluate_model()` function\n",
    "\n",
    "- **Params:**\n",
    "  - `model` -> Fine-tuned model path\n",
    "  - `df` -> Preprocessed data\n",
    "- **Returns:**\n",
    "  - `metrics` -> 8 different evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, df):\n",
    "    # Encode all sentences\n",
    "    candidate_embeddings = model.encode(df['candidate_combined'].tolist())\n",
    "    ai_embeddings = model.encode(df['ai_combined'].tolist())\n",
    "\n",
    "    # Calculate various similarity/distance measures\n",
    "    cosine_scores = [cosine_similarity(c, a) for c, a in zip(candidate_embeddings, ai_embeddings)]\n",
    "    manhattan_scores = [-manhattan_distance(c, a) for c, a in zip(candidate_embeddings, ai_embeddings)]\n",
    "    euclidean_scores = [-euclidean_distance(c, a) for c, a in zip(candidate_embeddings, ai_embeddings)]\n",
    "    dot_product_scores = [dot_product(c, a) for c, a in zip(candidate_embeddings, ai_embeddings)]\n",
    "    \n",
    "    true_scores = df['similarity_score'].tolist()\n",
    "    \n",
    "    # Calculate Spearman correlations\n",
    "    cosine_spearman = spearmanr(true_scores, cosine_scores).correlation\n",
    "    manhattan_spearman = spearmanr(true_scores, manhattan_scores).correlation\n",
    "    euclidean_spearman = spearmanr(true_scores, euclidean_scores).correlation\n",
    "    dot_product_spearman = spearmanr(true_scores, dot_product_scores).correlation\n",
    "    \n",
    "    # Calculate other metrics using cosine scores\n",
    "    mse = mean_squared_error(true_scores, cosine_scores)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_scores, cosine_scores)\n",
    "    r2 = r2_score(true_scores, cosine_scores)\n",
    "    \n",
    "    metrics = {\n",
    "        \"Cosine Spearman\": cosine_spearman,\n",
    "        \"Manhattan Spearman\": manhattan_spearman,\n",
    "        \"Euclidean Spearman\": euclidean_spearman,\n",
    "        \"Dot Product Spearman\": dot_product_spearman,\n",
    "        \"Mean Squared Error\": mse,\n",
    "        \"Root Mean Squared Error\": rmse,\n",
    "        \"Mean Absolute Error\": mae,\n",
    "        \"R-squared Score\": r2\n",
    "    }\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the Model\n",
    "Specify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"all-mpnet-base-v2\"\n",
    "# model_name = \"all-distilroberta-v1\"\n",
    "# model_name = \"all-MiniLM-L12-v2\"\n",
    "# model_name = \"all-MiniLM-L6-v2\"\n",
    "model_name = \"multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "data_dir = os.path.join(os.path.abspath(''), os.pardir, 'data')\n",
    "df = pd.read_csv(os.path.join(data_dir, 'preprocessed_data.csv'))\n",
    "\n",
    "# Define model export/output path\n",
    "model_dir = os.path.join(\n",
    "    os.path.abspath(''), os.pardir, 'models')\n",
    "model_path = os.path.join(model_dir, f'fine-tuned_{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Spearman: 0.9672\n",
      "Manhattan Spearman: 0.9603\n",
      "Euclidean Spearman: 0.9551\n",
      "Dot Product Spearman: 0.9652\n",
      "Mean Squared Error: 0.0086\n",
      "Root Mean Squared Error: 0.0925\n",
      "Mean Absolute Error: 0.0702\n",
      "R-squared Score: 0.8805\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_path, device=device)\n",
    "metrics = evaluate_model(model, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
